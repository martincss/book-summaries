#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass extbook
\use_default_options true
\maintain_unincluded_children false
\begin_local_layout
Format 66
InsetLayout Flex:Code
    LyxType               charstyle
    LabelString           code
    LatexType             command
    LatexName             code
    Font
      Family              Typewriter
    EndFont
    Preamble
    \newcommand{\code}[1]{\texttt{#1}}
    EndPreamble
    InToc                 true
    HTMLTag               code
	ResetsFont true
End
\end_local_layout
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "utopia" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "default" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Hands-On Machine Learning: summary
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\grad}{\boldsymbol{\nabla}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\pdev}[2]{\dfrac{\partial#1}{\partial#2}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{chapter}{9}
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Artificial Neural Networks
\end_layout

\begin_layout Standard
Artificial neural networks (ANNs): Machine learning model inspired by networks
 of biological neurons.
 They are at the very core of Deep Learning: versatile, powerful and scalable.
\end_layout

\begin_layout Section
From Biological to Artificial Neurons
\end_layout

\begin_layout Standard
Reasons to believe this wave of interest in ANNs is more profound:
\end_layout

\begin_layout Itemize
Huge quantity of data available to train & ANNs outperform other ML technologies
 in large & complex problems
\end_layout

\begin_layout Itemize
Tremendous increase in computing power since the 90s + GPU & cloud platforms
\end_layout

\begin_layout Itemize
Training algorithms have improved (by small tweaks with huge impact)
\end_layout

\begin_layout Itemize
Theoretical limitations have been mostly rare in practice
\end_layout

\begin_layout Itemize
ANNs seem to have entered a virtuous cycle of funding & progress
\end_layout

\begin_layout Subsection
Biological neurons
\end_layout

\begin_layout Standard
Individual biological neurons seem to behave in a simple way, but are organized
 in a network of billions, each connected to thousands of others.
 Highly complex computations can emerge from their combines efforts.
 It appears that biological neurons are organized in consecutive layers,
 especially in the cerebral cortex.
\end_layout

\begin_layout Subsection
Logical computations with neurons
\end_layout

\begin_layout Standard
The first artificial neurons had only binary inputs & one binary output.
 Each neuron's output is active if more than a certain number of inputs
 are.
 A network like this can compute complex logical expressions.
\end_layout

\begin_layout Subsection
The Perceptron
\end_layout

\begin_layout Standard
The 
\emph on
perceptron
\emph default
 is a simple ANN architecture composed by a single layer of 
\bar under
threshold logic unit
\bar default
 neurons.
 Their inputs and output are now real numbers; each input connection is
 associated with a weight.
 The TLU computes a weighted sum of its inputs 
\begin_inset Formula $\left(z=\boldsymbol{x}^{\dagger}\boldsymbol{w}\right)$
\end_inset

 and then applies a 
\emph on
step function
\emph default
.
 Its output is thus 
\begin_inset Formula $h_{\boldsymbol{w}}\left(\boldsymbol{x}\right)=\text{step}\left(z\right)$
\end_inset

.
\end_layout

\begin_layout Standard
A single TLU can be used for simple linear binary classification.
 The perceptron contains a layer of TLUs.
 each 
\emph on
fully connected
\emph default
 to all inputs and also containing a 
\emph on
bias
\emph default
 term.
 Ir can thus perform multioutput binary classification: 
\begin_inset Formula $h_{\boldsymbol{w},i}=\phi\left(\boldsymbol{X}\boldsymbol{W}+\boldsymbol{b}\right)$
\end_inset

, where 
\begin_inset Formula $\phi$
\end_inset

 is the activation function (step for TLUs), 
\series bold

\begin_inset Formula $\boldsymbol{X}$
\end_inset

 
\series default
is the input matrix 
\begin_inset Formula $\left(n_{\text{instances}}\times n_{\text{features}}\right)$
\end_inset

, 
\begin_inset Formula $\boldsymbol{W}$
\end_inset

 is the weights matrix 
\begin_inset Formula $\left(n_{\text{features}}\times n_{\text{neurons}}\right)$
\end_inset

, and 
\begin_inset Formula $\boldsymbol{b}$
\end_inset

 is the bias vector 
\begin_inset Formula $\left(n_{\text{neurons}}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The training is done in a way to reinforce connections that help reduce
 the error.
 The perceptron recieves one training instance at a time, and for each wrong
 output neuron, reinforces weights from inputs that would have contributed
 to the correct prediction: 
\begin_inset Formula 
\[
w_{i,j}^{\text{next}}=w_{i,j}+\underset{\overbrace{\text{learning rate}}}{\eta}\underset{\overbrace{\text{0 or 1}}}{\left(y_{j}-\hat{y}_{j}\right)x_{i}}
\]

\end_inset


\begin_inset Newline newline
\end_inset

If the training instances are 
\emph on
linearly separable
\emph default
, the algorithm is proven to converge to a (non unique) solution.
 Contrary to logistic regression, these perceptrons do not output a class
 probability.
\end_layout

\begin_layout Standard
Limitations of perceptrons (such as the XOR problem) can be eliminated by
 stacking multiple layers
\end_layout

\begin_layout Subsection
The Multilayer Perceptron and Backpropagation
\end_layout

\begin_layout Standard
A MLP is comprised of an input layer, an output layer and several 
\emph on
hidden
\emph default
 layers.
 Each layer except for the output) is fully connected (layers close to the
 input are called 
\emph on
lower
\emph default
).
 
\end_layout

\begin_layout Standard
The training algorithm is known as 
\series bold
backpropagation
\series default
: gradient descent with an efficient technique for 
\bar under
computing gradients automatically
\bar default
 (
\emph on
autodiff
\emph default
).
 In a forward and a backward pass, the algorithm is able to compute the
 gradient with respect to every model parameter.
\end_layout

\begin_layout Paragraph
Algorithm breakdown
\end_layout

\begin_layout Enumerate
Handles one mini-batch at a time, and goes through full training set multiple
 times (each pass is called an 
\emph on
epoch
\emph default
).
\end_layout

\begin_layout Enumerate
For each instance in the mini-batch, the output is computed in a 
\emph on
forward-pass
\emph default
 (intermediate results from all layers are preserved since they are needed
 for the backward pass).
\end_layout

\begin_layout Enumerate
The output error is measured using the loss function
\end_layout

\begin_layout Enumerate
Then it computes each output connection's contribution to the error.
 This is done analytically using the chain rule.
\end_layout

\begin_layout Enumerate
It now calculates the error contributions from the weights on the layer
 below (also with the chain rule); this is repeated working backwards to
 the input layer
\end_layout

\begin_layout Enumerate
Finally, a gradient step is performed to update all weights
\end_layout

\begin_layout Standard
It is important that all weights be initialized randomly, or else the training
 will fail (a 
\emph on
symmetry breaking
\emph default
 is required).
\end_layout

\begin_layout Standard
In order for the algorithm to work, the step function must be smoothed to
 have a well-defined non-zero derivative, thus allowing GD to make some
 progress at every step.
 A first replacement is the sigmoid function 
\begin_inset Formula $\sigma\left(z\right)=\dfrac{1}{1+\exp\left(-z\right)}$
\end_inset


\end_layout

\begin_layout Subparagraph
Other choices:
\end_layout

\begin_layout Itemize
\begin_inset Formula $\tanh\left(z\right)=2\sigma\left(2z\right)-1$
\end_inset

; similar to the logistic, but with output in 
\begin_inset Formula $[-1,1]$
\end_inset

.
 This tends to make each output mostly centered around 
\begin_inset Formula $0$
\end_inset

 at the beginning of training, which helps speed up the convergence.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\text{ReLU}\left(z\right)=\max\left(0,z\right)$
\end_inset

; derivative jumps at 
\begin_inset Formula $z=0$
\end_inset

, which can make GD bounce; but works well and is very fast to compute.
 Its unbounded image helps reduce some issues during GD.
\end_layout

\begin_layout Standard
Nonlinearities introduced by activation functions are essential to the complexit
y of the model.
 A large enough DNN can theoretically approximate any continuous function.
\end_layout

\begin_layout Subsection
Regression MLPs
\end_layout

\begin_layout Standard
One output neuron per output dimension (
\emph on
e.g.: 
\emph default
2 for locating the center of an object in an image; another 2 for a bounding
 box (height and width)).
 Usually no activation at the output, except for ReLU/
\begin_inset Formula $\text{softplus}\left(z\right)=\log\left[1+\exp\left(z\right)\right]$
\end_inset

 to restrict it to positive images; or 
\begin_inset Formula $\sigma$
\end_inset

/tanh to bound it.
\end_layout

\begin_layout Standard
Training loss is typically MSE, or MAE/Huber (a combination of both) if
 there are many training outliers.
 The number of hidden layers is usually 
\begin_inset Formula $\sim1-5$
\end_inset

, and the neurons per layer are 
\begin_inset Formula $\sim10-100$
\end_inset

.
\end_layout

\begin_layout Subsection
Classification MLPs
\end_layout

\begin_layout Standard
For each binary classification, a single output neuron with logistic activation
 is used, which can be interpreted as the estimated probability of the positive
 class.
\end_layout

\begin_layout Standard
For single output multiclass classification, an output neuron per class
 is needed, and a softmax activation for the whole layer (which estimates
 each class' probability)
\end_layout

\begin_layout Standard
The training loss function is generally multiclass cross-entropy.
 The rest of the architecture is broadly the same as with regression.
\end_layout

\begin_layout Section
Implementing MLPs with Keras
\end_layout

\begin_layout Standard
Keras is a high-level Deep learning API with several backends.
 TensorFlow comes bundled with its own implementation, 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
tf.keras
\end_layout

\end_inset

, which has many advantages (
\emph on
e.g.

\emph default
 TF's Data API to load and preprocess data efficiently).
\end_layout

\begin_layout Subsection
Building an image classifier using the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Sequential
\end_layout

\end_inset

 API
\end_layout

\begin_layout Standard
We use Fashion MNIST (70k grayscale 
\begin_inset Formula $28\times28$
\end_inset

 images, 10 classes).
 Since we will be using GD, we scale the input features.
\end_layout

\begin_layout Paragraph
Creating the model
\end_layout

\begin_layout Standard
\begin_inset Flex Code
status open

\begin_layout Plain Layout
model = keras.models.Sequential()
\end_layout

\end_inset

 is the simplest model, for a single stack of layers connected sequentially
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.add(keras.layers.Flatten(input_shape = [28,28]))
\end_layout

\end_inset

 converts the image to a 1D array.
 As is the first layer, we must pass the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
input_shape
\end_layout

\end_inset

 (for a single instance)
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.add(keras.layers.Dense(#neurons, activation = 'relu'))
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Instead of adding layers by one, it is possible to pass them as a list to
 the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Sequential
\end_layout

\end_inset

 constructor.
\end_layout

\begin_layout Subsubsection
Inspection
\end_layout

\begin_layout Standard
\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.summary()
\end_layout

\end_inset

displays the layers (by name), along with their parameters.
 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.layers
\end_layout

\end_inset

 returns them as a list, but they can also be called by name as 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.get_layer('dense')
\end_layout

\end_inset

.
 Its parameters can be retrieved by 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
weights, biases = layer.get_weights()
\end_layout

\end_inset

.
 For a custom initialization, when creating the layer we can set 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
kernel_initializer
\end_layout

\end_inset

 or 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
bias_initializer
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsubsection
Compiling the model
\end_layout

\begin_layout Standard
The method must be called to specify the loss function and optimizer: 
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'sgd',
 metrics = ['accuracy'])
\end_layout

\end_inset

.
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
metrics
\end_layout

\end_inset

 arg corresponds the list of metrics to be computed during training and
 evalutation.
 The loss function here is due to exclusive classes and the class given
 by a single index (this is considered 
\emph on
sparse
\emph default
).
 If class were given by a one-hot vector instead, we use 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
categorical_crossentropy
\end_layout

\end_inset

.
 To convert sparse to one-hot, 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
keras.utils.to_categorical()
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Training and evaluating
\end_layout

\begin_layout Standard
\begin_inset Flex Code
status open

\begin_layout Plain Layout
history = model.fit(X_train, y_train, epochs, validation_data = (X_val,y_val))
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

With class imbalance, we can set the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
class_weigths
\end_layout

\end_inset

 (and even 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
sample_weight
\end_layout

\end_inset

) arguments in the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

 method.
\end_layout

\begin_layout Standard
The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

 method returns a 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
History
\end_layout

\end_inset

 object with the training parameters (
\begin_inset Flex Code
status open

\begin_layout Plain Layout
history.params
\end_layout

\end_inset

) and a dictrionary 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
history.history
\end_layout

\end_inset

 containing the loss and metrics computed during training.
 This can be easily put in a dataframe toplot the learning curve (the training
 curve should be shifted to the left by half an epoch).
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

 method resumes training from last state if called multiple times.
\end_layout

\begin_layout Standard
To increase performance, the first hyperparameter to be tuned should be
 the learning rate; if that doesn't help, changing the optimizer.
 After that, the architecture and activation functions.
 Finally, we can evaluate the model with 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.evaluate(X_test,y_test)
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Building complex models using the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Functional
\end_layout

\end_inset

 API
\end_layout

\begin_layout Standard
To build models with more complex topologies, multiple inputs or outputs,
 keras offers the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Functional
\end_layout

\end_inset

 API.
 Each layer must be defined as a separate object, specifying its input
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
input_ = keras.layers.Input(shape=[...])
\end_layout

\end_inset

 an 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Input
\end_layout

\end_inset

 object must be defined (even more than one if necessary)
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
hidden = Dense(neurons, activation = 'relu')(input_)
\end_layout

\end_inset

 the input to this layer is passed by calling it as a function
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
concat = keras.layers.Concatenate()([input_, hidden])
\end_layout

\end_inset

 layer which concatenates inputs
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
model = keras.Model(inputs = [input_], outputs = [output])
\end_layout

\end_inset

 we create the model, specifying input(s) and output(s).
 The model can then be compiled and trained.
\begin_inset Newline newline
\end_inset

With multiple inputs, the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

 method must be supplied with a tuple (or dictionary) of input matrices.
 
\begin_inset Newline newline
\end_inset

Multiple outputs may be needed in many cases:
\end_layout

\begin_layout Itemize
The task may demand it; as in object location and identification (regression
 for a bounding box, classification to identify).
\end_layout

\begin_layout Itemize
Multiple independent tasks on the same data; the network can learn features
 in the data that are useful across tasks.
\end_layout

\begin_layout Itemize
It can be used as a regularization technique, for example, by adding an
 auxiliary output from middle layers, to ensure the underlying part learn
 something useful.
\end_layout

\begin_layout Itemize
Each output needs its own loss function.
 These will be added to obtain the total loss used in training, The losses
 will be passed as a list to the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

 method, along with a list of 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
loss_weights
\end_layout

\end_inset

 to perform a weighted sum instead.
 As with inputs, a tuple of targets must be supplied.
\end_layout

\begin_layout Subsection
Using the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Subclassing
\end_layout

\end_inset

 API to build dynamic models
\end_layout

\begin_layout Standard
To create models with even greater flexibility (
\emph on
e.g.
 
\emph default
loops, varying shapes, dynamical behaviours), we may use the subclassing
 API:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python,basicstyle={\ttfamily},tabsize=4"
inline false
status open

\begin_layout Plain Layout

class CustomModel(keras.Model): 		#inherit from base class
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	def __init__(self, ..., **kwargs):	  
\end_layout

\begin_layout Plain Layout

		super().__init__(**kwargs) 	 #handles standard args(e.g.
 name)
\end_layout

\begin_layout Plain Layout

		# all layers should be created in the constructor
\end_layout

\begin_layout Plain Layout

		self.hidden = ...
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	def call(self, inputs):
\end_layout

\begin_layout Plain Layout

	# all computations performed here (input need not be created, 
\end_layout

\begin_layout Plain Layout

	# just passed to the call method)
\end_layout

\begin_layout Plain Layout

		...
\end_layout

\begin_layout Plain Layout

	return outputs
\end_layout

\end_inset

The extra flexibility's cost is that the model architecture is hidden in
 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
call
\end_layout

\end_inset

, so keras cannot easily inspect it, save it or clone it.
\begin_inset Newline newline
\end_inset

Keras models can be used like regular layers to combine them.
\end_layout

\begin_layout Subsection
Saving and restoring a model
\end_layout

\begin_layout Standard
With the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Sequential
\end_layout

\end_inset

 or 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Functional
\end_layout

\end_inset

 API, a model can be saved to HDF5 using 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.save(fname)
\end_layout

\end_inset

.
 This saves the architecture, parameters for every layer and optimizer.
\begin_inset Newline newline
\end_inset

It can be loaded using 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
keras.models.load_model
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

This won't work with subclassing, but model parameters can be saved with
 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
save_weights
\end_layout

\end_inset

 (also loaded with 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
load_weights
\end_layout

\end_inset

).
\end_layout

\begin_layout Subsection
Using Callbacks
\end_layout

\begin_layout Standard
The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

 method accepts a list of objects in the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
callbacks
\end_layout

\end_inset

 argument, which will be called at the start and end of training, or each
 epoch or batch.
\end_layout

\begin_layout Subsubsection
Model checkpoint
\end_layout

\begin_layout Standard
\begin_inset Flex Code
status open

\begin_layout Plain Layout
checkpoint_cb = keras.callbacks.ModelCheckpoint(fname, save_best_only=True)
\end_layout

\end_inset

 the callback saves the model at regular intervals during training (by default
 at the end of each epoch).
 
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
save_best_only=True
\end_layout

\end_inset

 will only make the checkpoint when the performance on the validation set
 is the best so far.
\end_layout

\begin_layout Subsubsection
Early Stopping
\end_layout

\begin_layout Standard
\begin_inset Flex Code
status open

\begin_layout Plain Layout
early_stopping_cb = keras.callbacks.EarlyStopping(patience)
\end_layout

\end_inset

 will interrupt training when it measures no progress on validation during
 a number of epochs (given by 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
patience
\end_layout

\end_inset

).
 The best model can be restored enabling 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
restore_best_weights=True
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsubsection
Custom Callbacks
\end_layout

\begin_layout Standard
We can create callbacks by inheriting from the base class
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python,basicstyle={\ttfamily},tabsize=4"
inline false
status open

\begin_layout Plain Layout

class CustomCallback(keras.callbacks.Callback):
\end_layout

\begin_layout Plain Layout

	def on_epoch_end(self, epoch, logs):
\end_layout

\begin_layout Plain Layout

		print(logs['val_loss'])
\end_layout

\end_inset

options include 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
on_train_end
\end_layout

\end_inset

, 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
on_batch_end
\end_layout

\end_inset

, etc.
 Also ones only with evaluate, predict.
\end_layout

\begin_layout Subsection
Using TensorBoard for visualization
\end_layout

\begin_layout Standard
TensorBoard is an interactive visualization tool to view learning curves
 during training, compare them between runs, visualize the computation graph,
 training statistics & multidimensional data projected to 3D (and more).
\begin_inset Newline newline
\end_inset

To use it, the model must generate binary 
\emph on
event files
\emph default
, The TB server monitors a log directory; it is useful to have a different
 subdir for each run.
 The TB callback must be used to generate the files:
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
tensorboard_cb = keras.callback.TensorBoard(run_logdir)
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

The server can be called with 
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
%load_ext tensorboard
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
%tensorboard –logdir=./my_logs –port=6006
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

TF also has a low-level API to manually write logs (
\begin_inset Flex Code
status open

\begin_layout Plain Layout
tf.summary
\end_layout

\end_inset

)
\end_layout

\begin_layout Section
Fine-tuning neural network hyperparameters
\end_layout

\begin_layout Standard
The main drawback of the flexibility of ANNs is the number of hyperparameters
 to tweak.
 There are some options to find the best combination.
\begin_inset Newline newline
\end_inset

One option is to try many combinations using grid or random search and measuring
 on the validation set (or using k-fold cross validation).
 To work with the scikit-learn API, we use a wrapper to treat models as
 sklearn estimators:
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)
\end_layout

\end_inset

 where 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
build_model
\end_layout

\end_inset

 is function that takes a set og hyperparameters and returns a compiled
 model.
\begin_inset Newline newline
\end_inset

Random search works well for fairly simple problems, but when training is
 slow, it will only explore a tiny portion of hyperparameter space.
\end_layout

\begin_layout Standard
Other approaches 
\begin_inset Quotes eld
\end_inset

zoom in
\begin_inset Quotes erd
\end_inset

 when a region in the space turns out to be good; there are several libraries
 for this purpose
\end_layout

\begin_layout Subsection
Number of hidden layers
\end_layout

\begin_layout Standard
For simple problems, a single hidden layer may be enough; but for complex
 problems, deep networks have a much higher 
\emph on
parameter efficiency
\emph default
 than shallow ones: they can model complex functions using exponentially
 fewer neurons.
\end_layout

\begin_layout Standard
Deep networks can model the hierarchical structure of data: lower layers
 encode low-level features, and higher layers combine them to model high-level,
 more abstract structures (
\emph on
e.g.

\emph default
 from line segments to faces).
 This can also help the model generalize to other datasets.
 For example, we may reuse the lower layers from a model already trained
 to recognize faces to train a new model to recognize hairstyles
\end_layout

\begin_layout Chapter
Training Deep Neural Networks
\end_layout

\begin_layout Standard
Training a DNN (10+ layers), each with 100s of neurons and 100000s of connection
s can incur many problems:
\end_layout

\begin_layout Itemize

\emph on
Vanishing/exploding gradients 
\emph default
problem, which make lower layers very hard to train.
\end_layout

\begin_layout Itemize
Not enough training data for such a large network, or too costly to label.
\end_layout

\begin_layout Itemize
Training may be extremely slow
\end_layout

\begin_layout Itemize
A model with millions of parameters would severely risk overfitting, if
 there are not enough training instances
\end_layout

\begin_layout Section
The Vanishing/exploding gradients problems
\end_layout

\begin_layout Standard
BP works by propagating the error gradient from the output to the input
 layer.
 However, gradients often get smaller as the algorithm progressed to the
 lower layers: thus the GD update is not significant and training cannot
 converge to a good solution.
 Likewise, the gradients can grow bigger and the algorithm diverges.
\begin_inset Newline newline
\end_inset

This behaviour was addressed in 
\begin_inset CommandInset citation
LatexCommand cite
key "glorot10"
literal "false"

\end_inset

.
 Using a sigmoid activation and 
\begin_inset Formula $\mathcal{N}\left(0,1\right)$
\end_inset

 weight initialization causes weight variance to grow with layer depth.
 causing the activations to saturate.
 This implies the gradient is close to 0 dilutes further with BP.
\end_layout

\begin_layout Subsection
Glorot and He initialization
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "glorot10"
literal "false"

\end_inset

, they argue that the variance of the outputs of each layer needs to be
 equal to the variance of its inputs, and the same condition is needed for
 the variance of the gradients before and after backflowing through the
 layer, for the signal to flow properly in both directions.
 This cannot be guaranteed when #inputs (
\emph on
fan-in
\emph default
) is not equal to the #outputs (
\emph on
fan-out
\emph default
), but a good compromise can be made
\end_layout

\begin_layout Subsubsection
Glorot/Xavier initialization (when using logistic activation)
\end_layout

\begin_layout Standard
Weights to be sampled from a normal distribution 
\begin_inset Formula $\mathcal{N}\left(0,\sigma^{2}\right)$
\end_inset

, with 
\begin_inset Formula $\sigma^{2}=\dfrac{1}{\text{fan}_{\text{avg}}}$
\end_inset

; or from a uniform distribution 
\begin_inset Formula $U[-r,r]$
\end_inset

, with 
\begin_inset Formula $r=\sqrt{\dfrac{3}{\text{fan}_{\text{avg}}}}$
\end_inset


\end_layout

\begin_layout Subsubsection
He initialization (when using ReLU & variants)
\end_layout

\begin_layout Standard
Same as Glorot, but using 
\begin_inset Formula $\sigma^{2}=\dfrac{2}{\text{fan}_{\text{in}}}$
\end_inset

, or uniform using 
\begin_inset Formula $r=\sqrt{3\sigma^{2}}$
\end_inset


\end_layout

\begin_layout Subsubsection
LeCun initialization (when using SELU):
\end_layout

\begin_layout Standard
Only normal distribution with 
\begin_inset Formula $\sigma^{2}=\dfrac{1}{\text{fan}_{\text{in}}}$
\end_inset


\end_layout

\begin_layout Standard
By default.
 keras uses uniform Glorot.
 When creating a layer, the initializer can be set with 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
kernel_initializer='he_normal'
\end_layout

\end_inset

, for instance.
 A custom initialization can be done with (e.g.
 He average)
\begin_inset Newline newline
\end_inset


\begin_inset listings
lstparams "language=Python,basicstyle={\ttfamily},tabsize=4"
inline false
status open

\begin_layout Plain Layout

he_avg_init=keras.initializers.VarianceScaling(scale=2.,
\end_layout

\begin_layout Plain Layout

								mode='fan_avg',
\end_layout

\begin_layout Plain Layout

								distribution='uniform')
\end_layout

\end_inset

and this object must be passed to the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
kernel_initializer
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsection
Nonsaturating activation functions
\end_layout

\begin_layout Standard
Due to the lack of saturation, ReLUs work better than sigmoid in DNNs.
 However, during training some neurons may 
\begin_inset Quotes eld
\end_inset

die
\begin_inset Quotes erd
\end_inset

, outputting nothing than 0, anulling their gradients too.
 This may be solved using a 
\emph on
leaky
\emph default
 ReLU, with a small slope 
\begin_inset Formula $\alpha$
\end_inset

 for 
\begin_inset Formula $z<0$
\end_inset

.
 This ensures the unitrs never 
\begin_inset Quotes eld
\end_inset

die
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "bingxu15"
literal "false"

\end_inset

 concluded leaky ReLUs always outperform strict ones, even working better
 with 
\begin_inset Formula $\alpha\sim0.2$
\end_inset

 instead of 
\begin_inset Formula $\sim0.01$
\end_inset

.
 The paper also evaluated the 
\emph on
randomized leaky
\emph default
 ReLU (setting a random 
\begin_inset Formula $\alpha$
\end_inset

 during training and fixing to an average during test), which performed
 fairly well and seemed to act as a regularizer.
\begin_inset Newline newline
\end_inset

The paper also evaluated the PReLU, where 
\begin_inset Formula $\alpha$
\end_inset

 is an hyperparameter.
 It is reported to outperform ReLUs on large image datasets, but runs the
 risk of overfitting on smaller ones.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "clevert2015"
literal "false"

\end_inset

 proposed the 
\emph on
exponential linear unit 
\emph default
(ELU) that outperformed ReLU in both training time and test score.
 ELU:
\end_layout

\begin_layout Itemize
Takes on negative values with 
\begin_inset Formula $z<0$
\end_inset

, which allows the unit to have an average output closer to 0, and helps
 with vanishing gradients.
 The asympote negative value is an hyperparameter.
\end_layout

\begin_layout Itemize
It has nonzero gradient for 
\begin_inset Formula $z<0$
\end_inset

, which helps avoid dead neurons.
\end_layout

\begin_layout Itemize
With the hyperparameter 
\begin_inset Formula $\alpha$
\end_inset

 it is smooth everywhere, helps speed up GD
\end_layout

\begin_layout Standard
The main drawback is the slower computation than ReLU; but its faster convergenc
e rate compensates for that during training (not so during evaluation).
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "klambauer2017"
literal "false"

\end_inset

 introduced the scaled ELU (SELU).
 The authors showed that a nn built only from a stack of dense layers, using
 only SELU will 
\emph on
self-normalize
\emph default
: the output of each layer will tend to preserve a mean of 0 and a variance
 of 1 during training, which solves both gradient problems.
 SELU often outperforms other activation functions, but there are a few
 conditions for self-normalization:
\end_layout

\begin_layout Enumerate
Input features must be standarized (mean 0, std 1)
\end_layout

\begin_layout Enumerate
Every hidden layer initialized with LeCun normal
\end_layout

\begin_layout Enumerate
Network architecture must be sequential.
 In RNN or other architectures, self-normalization cannot be guaranteed.
\end_layout

\begin_layout Standard
The paper only guarantees self-normalization if all layers are dense.
 but it has been noted that SELU can improve performance in CNNs.
\end_layout

\begin_layout Standard
In general, SELU > ELU > leaky ReLU > tanh > sigmoid.
 When caring about runtime, leaky ReLU may be preferred, even with default
 
\begin_inset Formula $\alpha=0.3$
\end_inset

 (keras).
 ELU may perform better than SELU if self-normalization is not possible.
 RReLU may be used when overfitting or PReLU with a huge training set.
\end_layout

\begin_layout Standard
In keras, leaky ReLU and PReLU must be added as layers:
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
keras.layers.LeakyReLU(alpha)
\end_layout

\end_inset

 or 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
PReLU()
\end_layout

\end_inset

.
 SELU can be set as 
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
Dense(units, activation='selu', kernel_initializer='lecun_normal')
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Batch normalization
\end_layout

\begin_layout Standard
The former strategies can reduce the danger of vanishing/exploding gradients
 at the beginning of training, there is no guarantee they won't come back
 during training.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "ioffe2015batch"
literal "false"

\end_inset

 proposed 
\series bold
batch normalization 
\series default
to address these problems.
 The technique adds a new operation as a layer, to be used before or after
 the activation on each hidden layer.
 Each input feature is zero-centered and normalized, then re-scaled and
 centered using two learnable parameters (per input per layer).
 The model can thus learn the optimal scale and mean of each of the layer's
 inputs.
\end_layout

\begin_layout Standard
The algorithm estimates the input's mean and std on each mini-batch:
\begin_inset Formula 
\begin{align*}
1\; & \boldsymbol{\mu}_{B}=\dfrac{1}{m_{B}}\sum_{i=1}^{m_{b}}\boldsymbol{x}^{(i)}\\
2\; & \boldsymbol{\sigma}_{B}^{2}=\dfrac{1}{m_{B}}\sum_{i=1}^{m_{B}}\left(\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{B}\right)^{2}\\
3\; & \boldsymbol{\hat{x}}^{(i)}=\dfrac{\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{B}}{\sqrt{\boldsymbol{\sigma}_{B}^{2}+\varepsilon}}\\
4\; & \boldsymbol{z}^{(i)}=\boldsymbol{\gamma}\otimes\boldsymbol{x}^{(i)}+\boldsymbol{\beta}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $m_{B}$
\end_inset

 is the mini-batch size, 
\begin_inset Formula $\varepsilon$
\end_inset

 is a smoothing term 
\begin_inset Formula $\sim10^{-5}$
\end_inset

, 
\begin_inset Formula $\boldsymbol{z}^{(i)}$
\end_inset

 is the BN output and 
\begin_inset Formula $\boldsymbol{\gamma}\;\boldsymbol{\beta}$
\end_inset

 are the output scale and shift parameters.
\end_layout

\begin_layout Standard
\begin_inset Formula $\boldsymbol{\mu}_{B}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{\sigma}_{B}^{2}$
\end_inset

 are estimated for each batch furing training.
 At test time, an estimation for the whole training set (accumulated during
 training) is used.
 Thus, four parameter vectors are learned: 
\begin_inset Formula $\boldsymbol{\gamma},\;\boldsymbol{\beta},\;\boldsymbol{\mu},\;\boldsymbol{\sigma}^{2}$
\end_inset

 (the latter two used only after training).
\end_layout

\begin_layout Standard
BN also acts as a regularizer, reducing the need for other techniques.
 However, it does add some complexity to the model and has longer runtime.
 Though after training it is possible to merge the BN with the previous
 layer (by combining the parameters into new weights and biases).
\end_layout

\begin_layout Standard
Training will be slower, as each epoch takes much mote time; but this is
 counterbalanced by faster convergence, sot it will take fewer epochs to
 reach the same performance.
\end_layout

\begin_layout Subsubsection
Implementation in keras
\end_layout

\begin_layout Standard
BN can be added as a layer, before or after each hidden layer's activation
 function 
\begin_inset Newline newline
\end_inset

(as 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
keras.layers.BatchNormaliztion()
\end_layout

\end_inset

).
\end_layout

\begin_layout Standard
There is some debate whether the layer should be added before or after the
 activation.
 To add them before, the activation must be unspecified on the hidden layers
 and added as a separate layer.
 This allows us to remove bias from those layers with 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
use_bias=False
\end_layout

\end_inset

, as BN already contains a shift.
\end_layout

\begin_layout Standard
The layer contains a few hyperparameters; namely the 
\series bold
momentum (p)
\series default
, used when updating the exponential moving averages (for 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

)
\begin_inset Formula 
\[
\boldsymbol{\begin{gathered}\hat{v}\end{gathered}
}\leftarrow\boldsymbol{\hat{v}}\times p+\boldsymbol{v}\times(1-p)
\]

\end_inset


\begin_inset Newline newline
\end_inset

where 
\begin_inset Formula $\boldsymbol{\hat{v}}$
\end_inset

 is the running average and 
\begin_inset Formula $\boldsymbol{v}$
\end_inset

 is the new value.
 
\begin_inset Formula $p$
\end_inset

 is usually very close to 1 (0.9 or 0.99) (closer for larger datasets).
\end_layout

\begin_layout Standard
Another hyperparameter is 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
axis
\end_layout

\end_inset

, which indicates which axis to normalize.
 Default is -1 (last axis), which works when input is 2D 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
[batch_size,features]
\end_layout

\end_inset

.
 Care should be taken when feeding unflattened 2D images, for instance.
\end_layout

\begin_layout Standard
The different beahaviour when training and predicting is given by the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
training
\end_layout

\end_inset

 arg in the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
call
\end_layout

\end_inset

 method.
 It is set to 1 when calling 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
BN is near ubiquitous now, but 
\begin_inset CommandInset citation
LatexCommand cite
key "zhang2019fixup"
literal "false"

\end_inset

 managed to train a very deep (10k layers) net without BN by using a 
\emph on
fixed-update
\emph default
 weight initialization scheme.
\end_layout

\begin_layout Subsection
Gradient clipping
\end_layout

\begin_layout Standard
Another technique to prevent exploding gradient is to limit the gradient
 value below a threshold during BP.
 This is most often used on RNNs, where BN is tricky, With keras,
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
keras.optimizers.SGD(clipvalue=1.0)
\end_layout

\end_inset

 each gradient component will be limited to 
\begin_inset Formula $\pm1.0$
\end_inset

 (which may change the direction).
 To scale by the 
\begin_inset Formula $\ell^{2}$
\end_inset

norm and preserve the direction, 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
clipnorm
\end_layout

\end_inset

 may be used instead.
\end_layout

\begin_layout Section
Reusing pretrained layers
\end_layout

\begin_layout Standard
Instead of training a large DNN from scratch, it is usually a better idea
 to find an existing one which accomplishes a similar task and reuse the
 lower layers (
\series bold
\emph on
transfer learning
\series default
\emph default
).
 This will speed up training and require significantly less training data.
\end_layout

\begin_layout Standard
Transfer learning will work best when the inputs have similar low-level
 features.
 The upper hidden layers of the original model are less likely to be useful,
 as the high-level features may differ significantly between tasks.
 The more similar the tasks are, the more layers will be reusable.
\end_layout

\begin_layout Standard
A first attemts would be to freeze (make their weights non trainable) all
 reused layers and train & evaluate the model.
 Then, unfreezing layers from the top, retraining and checking if performance
 improves.
 The more data available, the more layers we can unfreeze, It is useful
 to reduce learning rate when unfreezing, as to avoid wrecking the fine-tuned
 weights.
\end_layout

\begin_layout Subsection
Transfer learning with keras
\end_layout

\begin_layout Standard
Cloning a model to reuse:
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
model_A_clone = keras.models.clone_model(model_A)
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
model_A_clone.set_weights(model_A.get_weights())
\end_layout

\end_inset

 (weights must be set manually).
 Reusing all but the last layer:
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
model_B = Sequential(model_A.layers[:-1])
\end_layout

\end_inset

.
 Freezing layers:
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
for layer in model_B.layers: layer.trainable=False
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Model must be always compiled after freezing/unfreezing.
\end_layout

\begin_layout Standard
Transfer learning does not work very well with small dense networks, presumably
 because they learn few, very specific patterns.
 TL works best with deep CNN, which tend to learn more general feature detectors.
\end_layout

\begin_layout Subsection
Unsupervised pretraining
\end_layout

\begin_layout Standard
When tackling a complex task with few training data and no pretrained similar
 model, we may be able to perform 
\series bold
unsupervised pretraining:
\series default
 With a large amount of unlabeled training data, an autoencoder or GAN can
 be trained, and its lower layers reused for a supervised model (using an
 output layer on top), fine-tuned with available data.
\end_layout

\begin_layout Subsection
Pretraining on an auxiliary task
\end_layout

\begin_layout Standard
Also with little training data, another option is to first train a nn on
 an auxiliary task for which training data can be easily obtained or generated,
 and then reuse that model's lower layers for the actual task.
 This is done in NLP by training a model to predict missing words ins a
 large corpus (
\emph on
i.e.

\emph default
 word2vec).
\end_layout

\begin_layout Standard

\series bold
\emph on
Self-supervised learning
\series default
\emph default
 consists on automatically generating labels from the data itself, thus
 counted as unsupervised.
\end_layout

\begin_layout Section
Faster optimizers
\end_layout

\begin_layout Standard
A further way to speed up training is by a faster optimizer than SGD.
\end_layout

\begin_layout Subsection
Momentum optimization
\end_layout

\begin_layout Standard
Regular GD updates by 
\begin_inset Formula $\boldsymbol{\theta}\leftarrow\boldsymbol{\theta}-\eta\grad_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}\right)$
\end_inset

, responding only to the local; when that gradient is tiny, it goes slowly.
 Momentum optimization updates weights with a 
\emph on
momentum
\emph default
 vector 
\begin_inset Formula $\boldsymbol{m}$
\end_inset

, and uses the gradient for acceleration.
 It introduces the hyperparameter 
\begin_inset Formula $\beta$
\end_inset

 (the 
\emph on
momentum
\emph default
) as a friction, to prevent the momentum vector from growing too large (usually
 
\begin_inset Formula $\beta\sim0.9$
\end_inset

).
 The whole technique allows the optimizer to speed up and also escape plateaus
 much faster.
 It is implemented in keras by 
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
optimizer=keras.optimizers.SGD(r=0.001,momentum=0.9)
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Due to the momentum, the optimizer may overshoot a bit and oscillate by
 the minimum.
 The friction helps get rid of them and converge.
\end_layout

\begin_layout Subsection
Nesterov accelerated gradient
\end_layout

\begin_layout Standard
A small variant almost always faster: to compute the gradient not at current
 position 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 but slightly ahead 
\begin_inset Formula $\boldsymbol{\theta}+\beta\boldsymbol{m}$
\end_inset

:
\begin_inset Formula 
\begin{align*}
1\; & \boldsymbol{m}\leftarrow\beta\boldsymbol{m}-\eta\grad_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}+\beta\boldsymbol{m}\right)\\
2\; & \boldsymbol{\theta}\leftarrow\boldsymbol{\theta}+\boldsymbol{m}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

This small tweak works because in general the momentum vector will be pointing
 on the right direction, so it will be slightly more accurate to measure
 the gradient a bit farther in that direction.
 It can be enabled by passing 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
nesterov=True
\end_layout

\end_inset

.
 to SGD constructor.
\end_layout

\begin_layout Subsection
AdaGrad
\end_layout

\begin_layout Standard
As with the elongated bowl cost function, the local gradient may not point
 directly at the global minimum.
 This algorithm corrects its direction earlier by scaling down the gradient
 along the steepest dimensions.
\begin_inset Formula 
\begin{align*}
1\; & \boldsymbol{s}\leftarrow\boldsymbol{s}+\grad_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}\right)\otimes\grad_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}\right)\\
2\; & \boldsymbol{\theta}\leftarrow\boldsymbol{\theta}-\eta\grad_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}\right)\oslash\sqrt{\boldsymbol{s}+\varepsilon}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

The algorithm decays the learning rate, but faster for dimensions with steeper
 slopes 
\emph on
(
\series bold
adaptive learning rate
\series default
)
\series bold
\emph default
.
 
\series default
AdaGrad works well for simple quadratic problems, but often stops too early
 when training nn (the 
\begin_inset Formula $\eta$
\end_inset

 gets scaled down too much too soon).
\end_layout

\begin_layout Subsection
RMSProp
\end_layout

\begin_layout Standard
This algorithm fixes the problem ins AdaGrad by only accumulating the gradients
 from recent iterations (by using and exponential running average).
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
1\; & \boldsymbol{s}\leftarrow\beta\boldsymbol{s}+\left(1-\beta\right)\grad_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}\right)\otimes\grad_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}\right)\\
2\; & \boldsymbol{\theta}\leftarrow\boldsymbol{\theta}-\eta\grad_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}\right)\oslash\sqrt{\boldsymbol{s}+\varepsilon}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

This always performs much better than AdaGrad.
 In keras,
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
keras.optimizers.RMSProp(lr, rho)
\end_layout

\end_inset

 (
\begin_inset Flex Code
status open

\begin_layout Plain Layout
rho
\end_layout

\end_inset

 is 
\begin_inset Formula $\beta$
\end_inset

 here).
\end_layout

\begin_layout Subsection
Adam and Nadam optimization
\end_layout

\begin_layout Standard
Adam (
\emph on
adaptive moment estimation
\emph default
) combines the ideas of momentum and RMSProp.
 It tracks and exponential running average of past gradients and their squares;
 an estimation of their first and second moments.
\begin_inset Formula 
\begin{align*}
1\; & \boldsymbol{m}\leftarrow\beta_{1}\boldsymbol{m}-\left(1-\beta_{1}\right)\grad_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}\right)\\
2\; & \boldsymbol{s}\leftarrow\beta_{2}\boldsymbol{s}+\left(1-\beta_{2}\right)\grad_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}\right)\otimes\grad_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}\right)\\
3\; & \boldsymbol{\hat{m}}\leftarrow\dfrac{\boldsymbol{m}}{1-\beta_{1}^{t}}\\
4\; & \boldsymbol{\hat{s}}\leftarrow\dfrac{s}{1-\beta_{2}^{t}}\\
5\; & \boldsymbol{\theta}\leftarrow\boldsymbol{\theta}+\eta\boldsymbol{m}\oslash\sqrt{\boldsymbol{s}+\varepsilon}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

3&4 help boost 
\begin_inset Formula $\boldsymbol{m}$
\end_inset

 & 
\begin_inset Formula $\boldsymbol{s}$
\end_inset

 at the beginning of training, since both are initialized to 
\begin_inset Formula $0$
\end_inset

 and will be biased towards it.
 In keras, 
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
keras.optimizers.Adam(lr,beta_1,beta_2)
\end_layout

\end_inset

.
 As an adaptive learning rate, it requires less tuning of the hyperparameter
 
\begin_inset Formula $\eta$
\end_inset

.
\end_layout

\begin_layout Paragraph
Two variations of Adam:
\end_layout

\begin_layout Itemize

\series bold
AdaMax: 
\series default
Adam scales down the parameter updates by the 
\begin_inset Formula $\ell^{2}$
\end_inset

 norm (scaled) between each component's current and past gradients.
 AdaMax uses instead the 
\begin_inset Formula $\ell^{\infty}$
\end_inset

 norm; 
\emph on
i.e.

\emph default
 replaces step 
\series bold
2
\series default
 with 
\begin_inset Formula $\boldsymbol{s}\leftarrow\max\left(\beta_{2}\boldsymbol{s},\grad_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}\right)\right)$
\end_inset

 (component-wise) and drops step 
\series bold
4
\series default
.
 This can make the algorithm more stable but it really depends on the dataset.
\end_layout

\begin_layout Itemize

\series bold
Nadam: 
\series default
Adam + Nesterov; will often converge slightly faster.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Dozat2016"
literal "false"

\end_inset

 finds that Nadam generally outperforms Adam, but is sometimes outperformed
 by RMSProp.
\end_layout

\begin_layout Standard
Adaptive optimization methods often converge fast to a good solution, but
 
\begin_inset CommandInset citation
LatexCommand cite
key "wilson2017marginal"
literal "false"

\end_inset

 showed they can lead to solutions that generalize poorly on some datasets.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "bibliography"
options "apalike"

\end_inset


\end_layout

\end_body
\end_document

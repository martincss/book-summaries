#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass extbook
\use_default_options true
\maintain_unincluded_children false
\begin_local_layout
Format 66
InsetLayout Flex:Code
    LyxType               charstyle
    LabelString           code
    LatexType             command
    LatexName             code
    Font
      Family              Typewriter
    EndFont
    Preamble
    \newcommand{\code}[1]{\texttt{#1}}
    EndPreamble
    InToc                 true
    HTMLTag               code
	ResetsFont true
End
\end_local_layout
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "utopia" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "default" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Hands-On Machine Learning: summary
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\grad}{\boldsymbol{\nabla}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\pdev}[2]{\dfrac{\partial#1}{\partial#2}}
\end_inset


\end_layout

\begin_layout Chapter*
Chapter 10: Artificial Neural Networks
\end_layout

\begin_layout Standard
Artificial neural networks (ANNs): Machine learning model inspired by networks
 of biological neurons.
 They are at the very core of Deep Learning: versatile, powerful and scalable.
\end_layout

\begin_layout Section
From Biological to Artificial Neurons
\end_layout

\begin_layout Standard
Reasons to believe this wave of interest in ANNs is more profound:
\end_layout

\begin_layout Itemize
Huge quantity of data available to train & ANNs outperform other ML technologies
 in large & complex problems
\end_layout

\begin_layout Itemize
Tremendous increase in computing power since the 90s + GPU & cloud platforms
\end_layout

\begin_layout Itemize
Training algorithms have improved (by small tweaks with huge impact)
\end_layout

\begin_layout Itemize
Theoretical limitations have been mostly rare in practice
\end_layout

\begin_layout Itemize
ANNs seem to have entered a virtuous cycle of funding & progress
\end_layout

\begin_layout Subsection
Biological neurons
\end_layout

\begin_layout Standard
Individual biological neurons seem to behave in a simple way, but are organized
 in a network of billions, each connected to thousands of others.
 Highly complex computations can emerge from their combines efforts.
 It appears that biological neurons are organized in consecutive layers,
 especially in the cerebral cortex.
\end_layout

\begin_layout Subsection
Logical computations with neurons
\end_layout

\begin_layout Standard
The first artificial neurons had only binary inputs & one binary output.
 Each neuron's output is active if more than a certain number of inputs
 are.
 A network like this can compute complex logical expressions.
\end_layout

\begin_layout Subsection
The Perceptron
\end_layout

\begin_layout Standard
The 
\emph on
perceptron
\emph default
 is a simple ANN architecture composed by a single layer of 
\bar under
threshold logic unit
\bar default
 neurons.
 Their inputs and output are now real numbers; each input connection is
 associated with a weight.
 The TLU computes a weighted sum of its inputs 
\begin_inset Formula $\left(z=\boldsymbol{x}^{\dagger}\boldsymbol{w}\right)$
\end_inset

 and then applies a 
\emph on
step function
\emph default
.
 Its output is thus 
\begin_inset Formula $h_{\boldsymbol{w}}\left(\boldsymbol{x}\right)=\text{step}\left(z\right)$
\end_inset

.
\end_layout

\begin_layout Standard
A single TLU can be used for simple linear binary classification.
 The perceptron contains a layer of TLUs.
 each 
\emph on
fully connected
\emph default
 to all inputs and also containing a 
\emph on
bias
\emph default
 term.
 Ir can thus perform multioutput binary classification: 
\begin_inset Formula $h_{\boldsymbol{w},i}=\phi\left(\boldsymbol{X}\boldsymbol{W}+\boldsymbol{b}\right)$
\end_inset

, where 
\begin_inset Formula $\phi$
\end_inset

 is the activation function (step for TLUs), 
\series bold

\begin_inset Formula $\boldsymbol{X}$
\end_inset

 
\series default
is the input matrix 
\begin_inset Formula $\left(n_{\text{instances}}\times n_{\text{features}}\right)$
\end_inset

, 
\begin_inset Formula $\boldsymbol{W}$
\end_inset

 is the weights matrix 
\begin_inset Formula $\left(n_{\text{features}}\times n_{\text{neurons}}\right)$
\end_inset

, and 
\begin_inset Formula $\boldsymbol{b}$
\end_inset

 is the bias vector 
\begin_inset Formula $\left(n_{\text{neurons}}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The training is done in a way to reinforce connections that help reduce
 the error.
 The perceptron recieves one training instance at a time, and for each wrong
 output neuron, reinforces weights from inputs that would have contributed
 to the correct prediction: 
\begin_inset Formula 
\[
w_{i,j}^{\text{next}}=w_{i,j}+\underset{\overbrace{\text{learning rate}}}{\eta}\underset{\overbrace{\text{0 or 1}}}{\left(y_{j}-\hat{y}_{j}\right)x_{i}}
\]

\end_inset


\begin_inset Newline newline
\end_inset

If the training instances are 
\emph on
linearly separable
\emph default
, the algorithm is proven to converge to a (non unique) solution.
 Contrary to logistic regression, these perceptrons do not output a class
 probability.
\end_layout

\begin_layout Standard
Limitations of perceptrons (such as the XOR problem) can be eliminated by
 stacking multiple layers
\end_layout

\begin_layout Subsection
The Multilayer Perceptron and Backpropagation
\end_layout

\begin_layout Standard
A MLP is comprised of an input layer, an output layer and several 
\emph on
hidden
\emph default
 layers.
 Each layer except for the output) is fully connected (layers close to the
 input are called 
\emph on
lower
\emph default
).
 
\end_layout

\begin_layout Standard
The training algorithm is known as 
\series bold
backpropagation
\series default
: gradient descent with an efficient technique for 
\bar under
computing gradients automatically
\bar default
 (
\emph on
autodiff
\emph default
).
 In a forward and a backward pass, the algorithm is able to compute the
 gradient with respect to every model parameter.
\end_layout

\begin_layout Paragraph
Algorithm breakdown
\end_layout

\begin_layout Enumerate
Handles one mini-batch at a time, and goes through full training set multiple
 times (each pass is called an 
\emph on
epoch
\emph default
).
\end_layout

\begin_layout Enumerate
For each instance in the mini-batch, the output is computed in a 
\emph on
forward-pass
\emph default
 (intermediate results from all layers are preserved since they are needed
 for the backward pass).
\end_layout

\begin_layout Enumerate
The output error is measured using the loss function
\end_layout

\begin_layout Enumerate
Then it computes each output connection's contribution to the error.
 This is done analytically using the chain rule.
\end_layout

\begin_layout Enumerate
It now calculates the error contributions from the weights on the layer
 below (also with the chain rule); this is repeated working backwards to
 the input layer
\end_layout

\begin_layout Enumerate
Finally, a gradient step is performed to update all weights
\end_layout

\begin_layout Standard
It is important that all weights be initialized randomly, or else the training
 will fail (a 
\emph on
symmetry breaking
\emph default
 is required).
\end_layout

\begin_layout Standard
In order for the algorithm to work, the step function must be smoothed to
 have a well-defined non-zero derivative, thus allowing GD to make some
 progress at every step.
 A first replacement is the sigmoid function 
\begin_inset Formula $\sigma\left(z\right)=\dfrac{1}{1+\exp\left(-z\right)}$
\end_inset


\end_layout

\begin_layout Subparagraph
Other choices:
\end_layout

\begin_layout Itemize
\begin_inset Formula $\tanh\left(z\right)=2\sigma\left(2z\right)-1$
\end_inset

; similar to the logistic, but with output in 
\begin_inset Formula $[-1,1]$
\end_inset

.
 This tends to make each output mostly centered around 
\begin_inset Formula $0$
\end_inset

 at the beginning of training, which helps speed up the convergence.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\text{ReLU}\left(z\right)=\max\left(0,z\right)$
\end_inset

; derivative jumps at 
\begin_inset Formula $z=0$
\end_inset

, which can make GD bounce; but works well and is very fast to compute.
 Its unbounded image helps reduce some issues during GD.
\end_layout

\begin_layout Standard
Nonlinearities introduced by activation functions are essential to the complexit
y of the model.
 A large enough DNN can theoretically approximate any continuous function.
\end_layout

\begin_layout Subsection
Regression MLPs
\end_layout

\begin_layout Standard
One output neuron per output dimension (
\emph on
e.g.: 
\emph default
2 for locating the center of an object in an image; another 2 for a bounding
 box (height and width)).
 Usually no activation at the output, except for ReLU/
\begin_inset Formula $\text{softplus}\left(z\right)=\log\left[1+\exp\left(z\right)\right]$
\end_inset

 to restrict it to positive images; or 
\begin_inset Formula $\sigma$
\end_inset

/tanh to bound it.
\end_layout

\begin_layout Standard
Training loss is typically MSE, or MAE/Huber (a combination of both) if
 there are many training outliers.
 The number of hidden layers is usually 
\begin_inset Formula $\sim1-5$
\end_inset

, and the neurons per layer are 
\begin_inset Formula $\sim10-100$
\end_inset

.
\end_layout

\begin_layout Subsection
Classification MLPs
\end_layout

\begin_layout Standard
For each binary classification, a single output neuron with logistic activation
 is used, which can be interpreted as the estimated probability of the positive
 class.
\end_layout

\begin_layout Standard
For single output multiclass classification, an output neuron per class
 is needed, and a softmax activation for the whole layer (which estimates
 each class' probability)
\end_layout

\begin_layout Standard
The training loss function is generally multiclass cross-entropy.
 The rest of the architecture is broadly the same as with regression.
\end_layout

\begin_layout Section
Implementing MLPs with Keras
\end_layout

\begin_layout Standard
Keras is a high-level Deep learning API with several backends.
 TensorFlow comes bundled with its own implementation, 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
tf.keras
\end_layout

\end_inset

, which has many advantages (
\emph on
e.g.

\emph default
 TF's Data API to load and preprocess data efficiently).
\end_layout

\begin_layout Subsection
Building an image classifier using the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Sequential
\end_layout

\end_inset

 API
\end_layout

\begin_layout Standard
We use Fashion MNIST (70k grayscale 
\begin_inset Formula $28\times28$
\end_inset

 images, 10 classes).
 Since we will be using GD, we scale the input features.
\end_layout

\begin_layout Paragraph
Creating the model
\end_layout

\begin_layout Standard
\begin_inset Flex Code
status open

\begin_layout Plain Layout
model = keras.models.Sequential()
\end_layout

\end_inset

 is the simplest model, for a single stack of layers connected sequentially
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.add(keras.layers.Flatten(input_shape = [28,28]))
\end_layout

\end_inset

 converts the image to a 1D array.
 As is the first layer, we must pass the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
input_shape
\end_layout

\end_inset

 (for a single instance)
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.add(keras.layers.Dense(#neurons, activation = 'relu'))
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Instead of adding layers by one, it is possible to pass them as a list to
 the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Sequential
\end_layout

\end_inset

 constructor.
\end_layout

\begin_layout Subsubsection
Inspection
\end_layout

\begin_layout Standard
\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.summary()
\end_layout

\end_inset

displays the layers (by name), along with their parameters.
 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.layers
\end_layout

\end_inset

 returns them as a list, but they can also be called by name as 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.get_layer('dense')
\end_layout

\end_inset

.
 Its parameters can be retrieved by 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
weights, biases = layer.get_weights()
\end_layout

\end_inset

.
 For a custom initialization, when creating the layer we can set 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
kernel_initializer
\end_layout

\end_inset

 or 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
bias_initializer
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsubsection
Compiling the model
\end_layout

\begin_layout Standard
The method must be called to specify the loss function and optimizer: 
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'sgd',
 metrics = ['accuracy'])
\end_layout

\end_inset

.
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
metrics
\end_layout

\end_inset

 arg corresponds the list of metrics to be computed during training and
 evalutation.
 The loss function here is due to exclusive classes and the class given
 by a single index (this is considered 
\emph on
sparse
\emph default
).
 If class were given by a one-hot vector instead, we use 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
categorical_crossentropy
\end_layout

\end_inset

.
 To convert sparse to one-hot, 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
keras.utils.to_categorical()
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Training and evaluating
\end_layout

\begin_layout Standard
\begin_inset Flex Code
status open

\begin_layout Plain Layout
history = model.fit(X_train, y_train, epochs, validation_data = (X_val,y_val))
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

With class imbalance, we can set the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
class_weigths
\end_layout

\end_inset

 (and even 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
sample_weight
\end_layout

\end_inset

) arguments in the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

 method.
\end_layout

\begin_layout Standard
The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

 method returns a 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
History
\end_layout

\end_inset

 object with the training parameters (
\begin_inset Flex Code
status open

\begin_layout Plain Layout
history.params
\end_layout

\end_inset

) and a dictrionary 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
history.history
\end_layout

\end_inset

 containing the loss and metrics computed during training.
 This can be easily put in a dataframe toplot the learning curve (the training
 curve should be shifted to the left by half an epoch).
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

 method resumes training from last state if called multiple times.
\end_layout

\begin_layout Standard
To increase performance, the first hyperparameter to be tuned should be
 the learning rate; if that doesn't help, changing the optimizer.
 After that, the architecture and activation functions.
 Finally, we can evaluate the model with 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.evaluate(X_test,y_test)
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Building complex models using the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Functional
\end_layout

\end_inset

 API
\end_layout

\begin_layout Standard
To build models with more complex topologies, multiple inputs or outputs,
 keras offers the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Functional
\end_layout

\end_inset

 API.
 Each layer must be defined as a separate object, specifying its input
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
input_ = keras.layers.Input(shape=[...])
\end_layout

\end_inset

 an 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Input
\end_layout

\end_inset

 object must be defined (even more than one if necessary)
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
hidden = Dense(neurons, activation = 'relu')(input_)
\end_layout

\end_inset

 the input to this layer is passed by calling it as a function
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
concat = keras.layers.Concatenate()([input_, hidden])
\end_layout

\end_inset

 layer which concatenates inputs
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
model = keras.Model(inputs = [input_], outputs = [output])
\end_layout

\end_inset

 we create the model, specifying input(s) and output(s).
 The model can then be compiled and trained.
\begin_inset Newline newline
\end_inset

With multiple inputs, the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

 method must be supplied with a tuple (or dictionary) of input matrices.
 
\begin_inset Newline newline
\end_inset

Multiple outputs may be needed in many cases:
\end_layout

\begin_layout Itemize
The task may demand it; as in object location and identification (regression
 for a bounding box, classification to identify).
\end_layout

\begin_layout Itemize
Multiple independent tasks on the same data; the network can learn features
 in the data that are useful across tasks.
\end_layout

\begin_layout Itemize
It can be used as a regularization technique, for example, by adding an
 auxiliary output from middle layers, to ensure the underlying part learn
 something useful.
\end_layout

\begin_layout Itemize
Each output needs its own loss function.
 These will be added to obtain the total loss used in training, The losses
 will be passed as a list to the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

 method, along with a list of 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
loss_weights
\end_layout

\end_inset

 to perform a weighted sum instead.
 As with inputs, a tuple of targets must be supplied.
\end_layout

\begin_layout Subsection
Using the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Subclassing
\end_layout

\end_inset

 API to build dynamic models
\end_layout

\begin_layout Standard
To create models with even greater flexibility (
\emph on
e.g.
 
\emph default
loops, varying shapes, dynamical behaviours), we may use the subclassing
 API:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python,basicstyle={\ttfamily},tabsize=4"
inline false
status open

\begin_layout Plain Layout

class CustomModel(keras.Model): 		#inherit from base class
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	def __init__(self, ..., **kwargs):	  
\end_layout

\begin_layout Plain Layout

		super().__init__(**kwargs) 	 #handles standard args(e.g.
 name)
\end_layout

\begin_layout Plain Layout

		# all layers should be created in the constructor
\end_layout

\begin_layout Plain Layout

		self.hidden = ...
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	def call(self, inputs):
\end_layout

\begin_layout Plain Layout

	# all computations performed here (input need not be created, 
\end_layout

\begin_layout Plain Layout

	# just passed to the call method)
\end_layout

\begin_layout Plain Layout

		...
\end_layout

\begin_layout Plain Layout

	return outputs
\end_layout

\end_inset

The extra flexibility's cost is that the model architecture is hidden in
 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
call
\end_layout

\end_inset

, so keras cannot easily inspect it, save it or clone it.
\begin_inset Newline newline
\end_inset

Keras models can be used like regular layers to combine them.
\end_layout

\begin_layout Subsection
Saving and restoring a model
\end_layout

\begin_layout Standard
With the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Sequential
\end_layout

\end_inset

 or 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Functional
\end_layout

\end_inset

 API, a model can be saved to HDF5 using 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
model.save(fname)
\end_layout

\end_inset

.
 This saves the architecture, parameters for every layer and optimizer.
\begin_inset Newline newline
\end_inset

It can be loaded using 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
keras.models.load_model
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

This won't work with subclassing, but model parameters can be saved with
 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
save_weights
\end_layout

\end_inset

 (also loaded with 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
load_weights
\end_layout

\end_inset

).
\end_layout

\begin_layout Subsection
Using Callbacks
\end_layout

\begin_layout Standard
The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
fit
\end_layout

\end_inset

 method accepts a list of objects in the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
callbacks
\end_layout

\end_inset

 argument, which will be called at the start and end of training, or each
 epoch or batch.
\end_layout

\begin_layout Subsubsection
Model checkpoint
\end_layout

\begin_layout Standard
\begin_inset Flex Code
status open

\begin_layout Plain Layout
checkpoint_cb = keras.callbacks.ModelCheckpoint(fname, save_best_only=True)
\end_layout

\end_inset

 the callback saves the model at regular intervals during training (by default
 at the end of each epoch).
 
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
save_best_only=True
\end_layout

\end_inset

 will only make the checkpoint when the performance on the validation set
 is the best so far.
\end_layout

\begin_layout Subsubsection
Early Stopping
\end_layout

\begin_layout Standard
\begin_inset Flex Code
status open

\begin_layout Plain Layout
early_stopping_cb = keras.callbacks.EarlyStopping(patience)
\end_layout

\end_inset

 will interrupt training when it measures no progress on validation during
 a number of epochs (given by 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
patience
\end_layout

\end_inset

).
 The best model can be restored enabling 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
restore_best_weights=True
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsubsection
Custom Callbacks
\end_layout

\begin_layout Standard
We can create callbacks by inheriting from the base class
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python,basicstyle={\ttfamily},tabsize=4"
inline false
status open

\begin_layout Plain Layout

class CustomCallback(keras.callbacks.Callback):
\end_layout

\begin_layout Plain Layout

	def on_epoch_end(self, epoch, logs):
\end_layout

\begin_layout Plain Layout

		print(logs['val_loss'])
\end_layout

\end_inset

options include 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
on_train_end
\end_layout

\end_inset

, 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
on_batch_end
\end_layout

\end_inset

, etc.
 Also ones only with evaluate, predict.
\end_layout

\begin_layout Subsection
Using TensorBoard for visualization
\end_layout

\begin_layout Standard
TensorBoard is an interactive visualization tool to view learning curves
 during training, compare them between runs, visualize the computation graph,
 training statistics & multidimensional data projected to 3D (and more).
\begin_inset Newline newline
\end_inset

To use it, the model must generate binary 
\emph on
event files
\emph default
, The TB server monitors a log directory; it is useful to have a different
 subdir for each run.
 The TB callback must be used to generate the files:
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
tensorboard_cb = keras.callback.TensorBoard(run_logdir)
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

The server can be called with 
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
%load_ext tensorboard
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
%tensorboard –logdir=./my_logs –port=6006
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

TF also has a low-level API to manually write logs (
\begin_inset Flex Code
status open

\begin_layout Plain Layout
tf.summary
\end_layout

\end_inset

)
\end_layout

\begin_layout Section
Fine-tuning neural network hyperparameters
\end_layout

\begin_layout Standard
The main drawback of the flexibility of ANNs is the number of hyperparameters
 to tweak.
 There are some options to find the best combination.
\begin_inset Newline newline
\end_inset

One option is to try many combinations using grid or random search and measuring
 on the validation set (or using k-fold cross validation).
 To work with the scikit-learn API, we use a wrapper to treat models as
 sklearn estimators:
\begin_inset Newline newline
\end_inset


\begin_inset Flex Code
status open

\begin_layout Plain Layout
keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)
\end_layout

\end_inset

 where 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
build_model
\end_layout

\end_inset

 is function that takes a set og hyperparameters and returns a compiled
 model.
\begin_inset Newline newline
\end_inset

Random search works well for fairly simple problems, but when training is
 slow, it will only explore a tiny portion of hyperparameter space.
\end_layout

\begin_layout Standard
Other approaches 
\begin_inset Quotes eld
\end_inset

zoom in
\begin_inset Quotes erd
\end_inset

 when a region in the space turns out to be good; there are several libraries
 for this purpose
\end_layout

\begin_layout Subsection
Number of hidden layers
\end_layout

\begin_layout Standard
For simple problems, a single hidden layer may be enough; but for complex
 problems, deep networks have a much higher 
\emph on
parameter efficiency
\emph default
 than shallow ones: they can model complex functions using exponentially
 fewer neurons.
\end_layout

\begin_layout Standard
Deep networks can model the hierarchical structure of data: lower layers
 encode low-level features, and higher layers combine them to model high-level,
 more abstract structures (
\emph on
e.g.

\emph default
 from line segments to faces).
 This can also help the model generalize to other datasets.
 For example, we may reuse the lower layers from a model already trained
 to recognize faces to train a new model to recognize hairstyles
\end_layout

\end_body
\end_document
